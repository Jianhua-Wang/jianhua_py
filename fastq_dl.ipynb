{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-04T14:33:37.545634Z",
     "start_time": "2020-01-04T14:33:37.539761Z"
    }
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from urllib.request import urlopen\n",
    "from subprocess import call\n",
    "import pandas as pd\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try to get ENA FTP url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-04T02:17:58.769117Z",
     "start_time": "2020-01-04T02:17:58.759420Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_ENA_url(srr_id):\n",
    "    '''\n",
    "    infer the ENA FTP url according the SRR (or ERR) id,\n",
    "    if the file or files are archived by EBI, return the urls\n",
    "    else return [].\n",
    "    \n",
    "    >>> get_ENA_url('ERR2365269')\n",
    "    >>> ['ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR236/009/ERR2365269/ERR2365269_1.fastq.gz',\n",
    "         'ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR236/009/ERR2365269/ERR2365269_2.fastq.gz']\n",
    "    '''\n",
    "    if len(srr_id) == 10:\n",
    "        ena_ftp_url = f'ftp://ftp.sra.ebi.ac.uk/vol1/fastq/{srr_id[:6]}/00{srr_id[-1]}/{srr_id}/'\n",
    "    else:\n",
    "        ena_ftp_url = f'ftp://ftp.sra.ebi.ac.uk/vol1/fastq/{srr_id[:6]}/{srr_id}/'\n",
    "\n",
    "    try:\n",
    "        files = urlopen(ena_ftp_url).read().decode('utf-8').split()[8::9]\n",
    "        return [f'{ena_ftp_url}{x}' for x in files]\n",
    "    except:\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try to get url of original file from SRA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the new SRA, the original sequencing read file are also can be downloaded from Amazon S3 (i.e. ERR2365269). \n",
    "\n",
    "But this new feature is only avaliable for data submitted after later 2019. \n",
    "\n",
    "So I won't consider this approach in the short run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-04T01:51:29.224053Z",
     "start_time": "2020-01-04T01:51:29.177797Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_original_url(srr_id):\n",
    "    '''\n",
    "    Scrapy the url of original file.\n",
    "    \n",
    "    >>> get_original_url('ERR2365269')\n",
    "    >>> ['http://ftp.sra.ebi.ac.uk/vol1/run/ERR236/ERR2365269/capt-cardio-1-R1.fastq.bz2',\n",
    "         'http://ftp.sra.ebi.ac.uk/vol1/run/ERR236/ERR2365269/capt-cardio-1-R2.fastq.bz2']\n",
    "    '''\n",
    "    original_urls = []\n",
    "    response = requests.get(\n",
    "        f'https://trace.ncbi.nlm.nih.gov/Traces/sra/?run={srr_id}')\n",
    "    soup = BeautifulSoup(response.content, 'lxml')\n",
    "    for h2 in soup.select('h2'):\n",
    "        if h2.text == 'Original format':\n",
    "            original_urls = h2.parent.select('a')\n",
    "            if len(original_urls) == 0:\n",
    "                pass\n",
    "            else:\n",
    "                original_urls = [i.attrs['href'] for i in original_urls]\n",
    "            break\n",
    "    return original_urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download SRR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-04T02:17:37.142983Z",
     "start_time": "2020-01-04T02:17:37.137275Z"
    }
   },
   "outputs": [],
   "source": [
    "def download_srr(srr_id):\n",
    "    '''\n",
    "    if the file were archived by EBI, get the urls and download using axel with 20 threads.\n",
    "    else using fasterq-dump.\n",
    "    '''\n",
    "    urls = get_ENA_url(srr_id)\n",
    "    if len(urls) == 0:\n",
    "        print(f'Download {srr_id} using fasterq-dump')\n",
    "        call(\n",
    "            f'/f/jianhua/nankai-hic/GEO/sratoolkit.2.9.6-1-ubuntu64/bin/fasterq-dump --split-files {srr_id}',\n",
    "            shell=True)\n",
    "    else:\n",
    "        print(f'Download {srr_id} using axel from ENA')\n",
    "        for url in urls:\n",
    "            call(f'/f/jianhua/software/axel -n 20 -a {url}', shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-04T05:42:19.725715Z",
     "start_time": "2020-01-04T05:42:17.126917Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ftp://ftp.sra.ebi.ac.uk/vol1/fastq/SRR959/004/SRR9595574/SRR9595574.fastq.gz']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_ENA_url('SRR9595574')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download SRX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-04T13:00:17.636049Z",
     "start_time": "2020-01-04T13:00:17.623012Z"
    }
   },
   "outputs": [],
   "source": [
    "def download_srx(srx_id):\n",
    "    '''\n",
    "    Get the SRR id under SRX and download SRR.\n",
    "\n",
    "    Some SRXs contain more than one SRR (i.e. SRX5545333). Download the SRRs iteratively for these cases.\n",
    "    \n",
    "    [TODO] Merge the SRRs in the future.\n",
    "    '''\n",
    "    response = requests.get(f'https://www.ncbi.nlm.nih.gov/sra/?term={srx_id}')\n",
    "    soup = BeautifulSoup(response.content, 'lxml')\n",
    "\n",
    "    srr_id_list = [srr.text for srr in soup.select('table')[0].select('a')]\n",
    "\n",
    "    if len(srr_id_list) == 0:\n",
    "        print('Invalid SRX id!')\n",
    "    elif len(srr_id_list) == 1:\n",
    "        print(f'{srx_id} only has 1 run: {srr_id_list[0]}')\n",
    "        print(f'Donwload {srr_id_list[0]}')\n",
    "        download_srr(srr_id_list[0])\n",
    "    else:\n",
    "        print(f'There {len(srr_id_list)} runs in {srx_id}')\n",
    "        for ith, srr_id in enumerate(srr_id_list):\n",
    "            print(f'Download No.{ith+1}: {srr_id}')\n",
    "            download_srr(srr_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download SRP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download GSM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-04T14:07:06.160436Z",
     "start_time": "2020-01-04T14:07:06.151558Z"
    }
   },
   "outputs": [],
   "source": [
    "def download_gsm(gsm_id):\n",
    "    '''\n",
    "    get SRX from GSM and download SRR\n",
    "    '''\n",
    "    response = requests.get(\n",
    "        f'https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc={gsm_id}')\n",
    "    soup = BeautifulSoup(response.content, 'lxml')\n",
    "\n",
    "    srx_id = None\n",
    "    for a in soup.select('a'):\n",
    "        if a.text.startswith('SRX'):\n",
    "            srx_id = a.text\n",
    "\n",
    "    if srx_id:\n",
    "        print(f'{gsm_id} corresponds to {srx_id}')\n",
    "        download_srx(srx_id)\n",
    "    else:\n",
    "        print('Invalid GSM id!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download GSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-04T14:17:36.676141Z",
     "start_time": "2020-01-04T14:17:36.671988Z"
    }
   },
   "outputs": [],
   "source": [
    "gse_id = 'GSE89946'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-04T14:22:50.237752Z",
     "start_time": "2020-01-04T14:22:50.225965Z"
    }
   },
   "outputs": [],
   "source": [
    "def download_gse(gse_id):\n",
    "    '''\n",
    "    Scrapy GSM ids from https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=gse_id\n",
    "    and download GSM\n",
    "    \n",
    "    [TODO] Split by super GSE and sub GSE\n",
    "    '''\n",
    "    response = requests.get(\n",
    "        f'https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc={gse_id}')\n",
    "    soup = BeautifulSoup(response.content, 'lxml')\n",
    "\n",
    "    gsm_list = []\n",
    "    for a in soup.select('a'):\n",
    "        if a.text.startswith('GSM'):\n",
    "            gsm_list.append(a.text)\n",
    "\n",
    "    if len(gsm_list) == 0:\n",
    "        print('Invalid GSE id!')\n",
    "    elif len(gsm_list) == 1:\n",
    "        print(f'{gse_id} only has 1 sample: {gsm_list[0]}')\n",
    "        print(f'Search SRX for {gsm_list[0]}')\n",
    "        download_gsm(gsm_list[0])\n",
    "    else:\n",
    "        print(f'There {len(gsm_list)} runs in {gse_id}')\n",
    "        for ith, gsm_id in enumerate(gsm_list):\n",
    "            print(f'Search SRX for No.{ith+1}: {gsm_id}')\n",
    "            download_gsm(gsm_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-04T14:37:20.370502Z",
     "start_time": "2020-01-04T14:37:20.361437Z"
    }
   },
   "outputs": [],
   "source": [
    "def download_accession(accession):\n",
    "    if accession.startswith('SRR'):\n",
    "        download_srr(accession)\n",
    "    elif accession.startswith('SRX'):\n",
    "        download_srx(accession)\n",
    "    elif accession.startswith('GSM'):\n",
    "        download_gsm(accession)\n",
    "    elif accession.startswith('GSE'):\n",
    "        download_gse(accession)\n",
    "    else:\n",
    "        print('Invalid Accession!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_logo():\n",
    "    logo = '''\n",
    "==========================================================================================\n",
    "     \\033[1;33m/\\\\\\033[0m\n",
    "    \\033[1;33m/__\\\\\\033[0m\\033[1;31m\\\\\\033[0m            This is a python script for SRA and GEO fastq file downloading\n",
    "   \\033[1;33m/\\033[0m  \\033[1;31m---\\\\\\033[0m           Author: Jianhua Wang\n",
    "  \\033[1;33m/\\\\\\033[0m      \\033[1;31m\\\\\\033[0m          Date:   01-04-2020\n",
    " \\033[1;33m/\\033[0m\\033[1;32m/\\\\\\033[0m\\033[1;33m\\\\\\033[0m     \\033[1;31m/\\\\\\033[0m\n",
    " \\033[1;32m/  \\   /\\033[0m\\033[1;31m/__\\\\\\033[0m\n",
    "\\033[1;32m`----`-----\\033[0m\n",
    "==========================================================================================\n",
    "    '''\n",
    "    print(logo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def parseArguments():\n",
    "    parser = argparse.ArgumentParser(usage=\"conver genome build of txt or csv file, require pyliftover\",description=\"python liftover.py -c 0 -p 1 test.txt test_lifted.txt\",)\n",
    "    parser.add_argument('Accession', nargs='?', type=str, help='GEO or SRA Accession'),\n",
    "    parser.add_argument('-f','--file', type=str, help='Accession list text file',metavar=''),\n",
    "    args = parser.parse_args()\n",
    "    return args\n",
    "args = parseArguments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-04T14:45:14.627785Z",
     "start_time": "2020-01-04T14:45:14.619792Z"
    }
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    if args.file:\n",
    "        accession_list = open('./gse.txt','r')\n",
    "        accession_list = accession_list.readlines()\n",
    "        for accession in accession_list:\n",
    "            download_accession(accession.strip())\n",
    "    else:\n",
    "        download_accession(args.Accession)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    print_logo()\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "309.6px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
